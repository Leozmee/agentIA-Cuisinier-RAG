import chromadb
import os
from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_chroma import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
import gradio as gr

os.environ["CHROMA_ENABLE_TELEMETRY"] = "false"

# Configuration
COLLECTION_NAME = "pizzeria_collection"
EMBEDDING_MODEL = "mxbai-embed-large"
LLM_MODEL = "llama3.2:latest"

print("Initialisation des composants LangChain...")

# Initialisation
ollama_embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)
client = chromadb.PersistentClient(path="./chroma_db")

vectorstore = Chroma(
    client=client,
    collection_name=COLLECTION_NAME,
    embedding_function=ollama_embeddings
)

print(f"Collection charg√©e : {COLLECTION_NAME}")
print(f"Nombre de documents : {vectorstore._collection.count()}")

retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 10}  # Plus de r√©sultats
)

llm = ChatOllama(model=LLM_MODEL)

# Prompt am√©lior√© avec plus de contexte
template = """Tu es l'assistant IA de la pizzeria "Bella Napoli". Tu es sp√©cialis√© dans les informations sur les allerg√®nes et ingr√©dients de nos pizzas et produits.

CONTEXTE DOCUMENTAIRE :
{context}

INSTRUCTIONS D√âTAILL√âES :

üçï POUR LES PIZZAS :
- Nous avons 30+ pizzas : Margherita, Biod√©lice, V√©gane, Guiguitte (Jambon/Chorizo/Ch√®vre/Merguez/B≈ìuf), Cazimir, Super Regina, Italienne, Cow Boy, Croq Terroir, D√©couverte, etc.
- Chaque pizza a des allerg√®nes sp√©cifiques list√©s dans le contexte JSON
- R√©ponds avec les allerg√®nes exacts de la pizza demand√©e

ü•ò POUR LES INGR√âDIENTS :
- Analyse les tableaux d'allerg√®nes o√π "P" = pr√©sence d'allerg√®ne
- Ingr√©dients disponibles : Curry, Cr√®me fra√Æche, Mozzarella, Saumon fum√©, Chorizo, Jambon, etc.
- Tous les fromages contiennent du "Lait"
- La p√¢te √† pizza contient toujours "C√©r√©ales contenant du gluten"

üìã ALLERG√àNES OFFICIELS (14 majeurs UE) :
1. C√©r√©ales contenant du gluten (bl√©, seigle, orge, avoine, √©peautre, kamut)
2. Crustac√©s 3. ≈íufs 4. Poissons 5. Arachides 6. Soja
7. Lait (y compris lactose) 8. Fruits √† coques 9. C√©leri 10. Moutarde
11. Graines de S√©same 12. Anhydrides sulfureux et Sulfites 13. Lupin 14. Mollusques

üéØ R√àGLES DE R√âPONSE :
- Sois TR√àS pr√©cis sur les allerg√®nes pr√©sents
- Si c'est une pizza, donne la liste compl√®te des allerg√®nes
- Si c'est un ingr√©dient, cherche dans le tableau avec les "P"
- Mentionne toujours la source (JSON pour pizzas, tableau pour ingr√©dients)
- Ajoute les notes importantes si pertinent (traces possibles, environnement gluten)

‚ö†Ô∏è S√âCURIT√â ALIMENTAIRE :
- Ces informations concernent les allerg√®nes volontairement incorpor√©s
- Traces possibles d'autres allerg√®nes lors de la fabrication
- Pizza "sans gluten" pr√©par√©e dans environnement contenant du gluten
- En cas de doute grave, recommande de contacter directement la pizzeria

QUESTION : {question}

R√âPONSE (pr√©cise, s√©curis√©e et professionnelle) :"""

prompt = ChatPromptTemplate.from_template(template)

def search_ingredient_comprehensive(ingredient_name):
    """Recherche exhaustive d'un ingr√©dient"""
    try:
        # 1. Recherche vectorielle
        vector_results = vectorstore.similarity_search(ingredient_name, k=10)
        
        # 2. Recherche directe dans tous les documents
        all_docs = vectorstore._collection.get()
        direct_results = []
        
        for doc in all_docs['documents']:
            if ingredient_name.lower() in doc.lower():
                # Extraire les lignes pertinentes
                lines = doc.split('\n')
                for i, line in enumerate(lines):
                    if ingredient_name.lower() in line.lower():
                        # Prendre aussi les lignes autour pour le contexte
                        context_lines = []
                        for j in range(max(0, i-2), min(len(lines), i+3)):
                            context_lines.append(lines[j])
                        direct_results.append('\n'.join(context_lines))
        
        return vector_results, direct_results
        
    except Exception as e:
        print(f"Erreur recherche : {e}")
        return [], []

def answer_question_improved(question):
    """R√©ponse am√©lior√©e avec recherche exhaustive"""
    try:
        # Identifier l'ingr√©dient dans la question
        keywords = {
            "cr√®me fra√Æche": ["cr√®me fra√Æche", "creme fraiche", "cr√®me"],
            "mozzarella": ["mozzarella", "mozza"],
            "curry": ["curry"],
            "sauce tomate": ["sauce tomate", "tomate"],
            "p√¢te": ["p√¢te", "pate"],
            "fromage": ["fromage"],
            "bacon": ["bacon"],
            "jambon": ["jambon"],
            "emmental": ["emmental"],
            "gorgonzola": ["gorgonzola"],
            "parmigiano": ["parmigiano", "parmesan"],
            "reblochon": ["reblochon"],
            "raclette": ["raclette"]
        }
        
        ingredient_found = None
        for ingredient, variations in keywords.items():
            if any(var in question.lower() for var in variations):
                ingredient_found = ingredient
                break
        
        if ingredient_found:
            print(f"üîç Recherche pour : {ingredient_found}")
            
            # Recherche exhaustive
            vector_results, direct_results = search_ingredient_comprehensive(ingredient_found)
            
            print(f"üìä R√©sultats vectoriels : {len(vector_results)}")
            print(f"üìä R√©sultats directs : {len(direct_results)}")
            
            # Construire le contexte enrichi
            all_context = []
            
            # Ajouter r√©sultats vectoriels
            for doc in vector_results:
                all_context.append(doc.page_content)
            
            # Ajouter r√©sultats directs
            all_context.extend(direct_results[:5])  # Limiter pour √©viter trop de texte
            
            if all_context:
                # Cr√©er prompt enrichi
                enriched_context = "\n\n--- DOCUMENT ---\n".join(all_context)
                
                enriched_prompt = f"""Tu es l'assistant IA de la pizzeria "Bella Napoli".

CONTEXTE COMPLET SUR {ingredient_found.upper()} :
{enriched_context}

QUESTION : {question}

INSTRUCTIONS :
- Cherche sp√©cifiquement "{ingredient_found}" dans le contexte
- Si tu vois un tableau, trouve la ligne avec "{ingredient_found}"
- Note tous les "P" dans les colonnes d'allerg√®nes pour cette ligne
- Liste clairement tous les allerg√®nes pr√©sents (marqu√©s "P")

R√âPONSE :"""
                
                try:
                    answer = llm.invoke(enriched_prompt).content
                    return answer
                except Exception as e:
                    print(f"Erreur LLM : {e}")
            
            # Si aucun contexte trouv√©, utiliser les r√©sultats bruts
            if direct_results:
                return f"Informations trouv√©es sur {ingredient_found} :\n\n" + "\n\n".join(direct_results[:3])
        
        # Fallback sur RAG standard
        return rag_chain.invoke(question)
        
    except Exception as e:
        return f"Erreur : {e}"

# Construction de la cha√Æne RAG standard
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)


def chat_with_bot(message, history):
    """Fonction pour l'interface Gradio avec historique - Format messages"""
    if not message.strip():
        return history, history
    
    response = answer_question_improved(message)
    
    # Format moderne pour Gradio : dictionnaires avec 'role' et 'content'
    history.append({"role": "user", "content": message})
    history.append({"role": "assistant", "content": response})
    
    return history, history

def test_search_gradio():
    """Test de recherche pour l'interface Gradio"""
    try:
       
        all_docs = vectorstore._collection.get()
        creme_docs = []
        
        for i, doc in enumerate(all_docs['documents']):
            if "cr√®me fra√Æche" in doc.lower() or "creme fraiche" in doc.lower():
                creme_docs.append((i, doc))
        
        result = f"Documents contenant 'cr√®me fra√Æche' : {len(creme_docs)}\n\n"
        
        for i, (doc_idx, doc) in enumerate(creme_docs[:2]):
            result += f"üìÑ Document {doc_idx} :\n"
            lines = doc.split('\n')
            for line in lines:
                if "cr√®me" in line.lower() and len(line.strip()) > 5:
                    result += f"   {line.strip()}\n"
            result += "\n"
                    
        return result
        
    except Exception as e:
        return f"Erreur test : {e}"

# ‚úÖ INTERFACE GRADIO INT√âGR√âE

def create_gradio_interface():
    """Interface Gradio compl√®te"""
    with gr.Blocks(
        title="üçï Assistant Pizzeria Bella Napoli", 
        theme=gr.themes.Soft(),
        css="""
        .gradio-container {
            max-width: 1200px !important;
        }
        """
    ) as demo:
        
        gr.Markdown("""
        # üçï Assistant Pizzeria Bella Napoli
        ### Votre assistant sp√©cialis√© en allerg√®nes et ingr√©dients
        
        Posez vos questions sur les allerg√®nes des pizzas, sauces et ingr√©dients !
        """)
        
        with gr.Tabs():
            with gr.TabItem("üí¨ Chat Assistant"):
                chatbot = gr.Chatbot(
                    label="Conversation avec l'assistant",
                    height=500,
                    placeholder="Posez votre question sur les allerg√®nes...",
                    show_copy_button=True,
                    type="messages"  
                )
                
                with gr.Row():
                    msg = gr.Textbox(
                        label="Votre question",
                        placeholder="Ex: La cr√®me fra√Æche contient quels allerg√®nes ?",
                        scale=4,
                        lines=2
                    )
                    with gr.Column(scale=1):
                        send_btn = gr.Button("üì§ Envoyer", variant="primary")
                        clear_btn = gr.Button("üóëÔ∏è Effacer", variant="secondary")
                
                gr.Examples(
                    examples=[
                        "La cr√®me fra√Æche contient quels allerg√®nes ?",
                        "Quels sont les allerg√®nes de la mozzarella ?",
                        "La sauce curry a-t-elle du gluten ?",
                        "Y a-t-il du lait dans la p√¢te √† pizza ?",
                        "Quels fromages sont sans lactose ?",
                        "Le jambon contient-il des sulfites ?",
                        "Y a-t-il des fruits √† coque dans les sauces ?"
                    ],
                    inputs=msg,
                    label="Exemples de questions"
                )
                
                msg.submit(chat_with_bot, [msg, chatbot], [chatbot, chatbot])
                send_btn.click(chat_with_bot, [msg, chatbot], [chatbot, chatbot])
                clear_btn.click(lambda: [], outputs=[chatbot, chatbot])
            
                msg.submit(lambda: "", outputs=msg)
                send_btn.click(lambda: "", outputs=msg)
            
            with gr.TabItem("üîß Test & Debug"):
                gr.Markdown("### Outils de test et diagnostic")
                
                with gr.Row():
                    test_btn = gr.Button("üß™ Tester recherche cr√®me fra√Æche", variant="secondary")
                    info_btn = gr.Button("‚ÑπÔ∏è Infos syst√®me", variant="secondary")
                
                test_output = gr.Textbox(
                    label="R√©sultats des tests",
                    lines=15,
                    interactive=False
                )
                
                def get_system_info():
                    """Obtenir les informations syst√®me"""
                    try:
                        doc_count = vectorstore._collection.count()
                        info = f"""
üìä INFORMATIONS SYST√àME
=======================

‚úÖ Collection : {COLLECTION_NAME}
‚úÖ Nombre de documents : {doc_count}
‚úÖ Mod√®le d'embedding : {EMBEDDING_MODEL}
‚úÖ Mod√®le LLM : {LLM_MODEL}
‚úÖ Recherche : Top {retriever.search_kwargs.get('k', 10)} r√©sultats

üîç MOTS-CL√âS RECONNUS :
- cr√®me fra√Æche, mozzarella, curry
- sauce tomate, p√¢te, fromage
- bacon, jambon, emmental
- gorgonzola, parmigiano, reblochon, raclette

üí° CONSEILS D'UTILISATION :
- Soyez sp√©cifique dans vos questions
- Mentionnez l'ingr√©dient exact
- Utilisez les exemples pour vous guider
                        """
                        return info
                    except Exception as e:
                        return f"Erreur : {e}"
                
                test_btn.click(test_search_gradio, outputs=test_output)
                info_btn.click(get_system_info, outputs=test_output)
        
        # Instructions d'utilisation
        gr.Markdown("""
        ---
        **üí° Instructions d'utilisation :**
        
        1. **Chat Principal** : Posez vos questions sur les allerg√®nes des ingr√©dients
        2. **Test & Debug** : V√©rifiez le fonctionnement du syst√®me
        3. **Exemples** : Cliquez sur les exemples pour voir le format de questions
        
        **üéØ Types de questions support√©es :**
        - "Quels allerg√®nes contient [ingr√©dient] ?"
        - "[Ingr√©dient] a-t-il du [allerg√®ne] ?"
        - "Y a-t-il du [allerg√®ne] dans [ingr√©dient] ?"
        
        **‚ö†Ô∏è Allerg√®nes surveill√©s :** Gluten, Lait, ≈íufs, Fruits √† coque, Soja, etc.
        """)
    
    return demo

# Point d'entr√©e principal
if __name__ == "__main__":
    print("\nüöÄ Assistant Pizzeria Bella Napoli")
    print("=" * 50)
    
    # V√©rification rapide du syst√®me
    try:
        doc_count = vectorstore._collection.count()
        if doc_count > 0:
            print(f"‚úÖ Base de donn√©es charg√©e : {doc_count} documents")
            print("üöÄ Lancement de l'interface Gradio...")
            
            # Cr√©er et lancer l'interface Gradio
            demo = create_gradio_interface()
            demo.launch(
                share=False,
                server_name="127.0.0.1",
                server_port=7860,
                show_error=True
                # show_tips supprim√© car non support√©
            )
        else:
            print("‚ùå Aucun document dans la base de donn√©es")
            print("‚û°Ô∏è Cr√©ez d'abord la base avec le script de cr√©ation")
            
    except Exception as e:
        print(f"‚ùå Erreur : {e}")
        print("‚û°Ô∏è V√©rifiez que la base de donn√©es existe")